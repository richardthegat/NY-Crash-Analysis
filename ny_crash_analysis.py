# -*- coding: utf-8 -*-
"""NY Crash Analysis

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zy_3f7LMVfcotb5Ngu-zTlkaufqEiQZ-
"""

# Import relevant libraries
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import folium
from folium.plugins import HeatMap
from statsmodels.tsa.seasonal import seasonal_decompose

# Description of libraries

# Pandas: This library is used for data manipulation and analysis.
# Matplotlib: This library is used for creating visualizations in Python.
# Seaborn: This library is used for creating statistical data visualizations based on Matplotlib.
# Folium: This library is used for creating interactive maps in Python.


# Milestone 1: Data Preparation

# Mount the drive and load the file
from google.colab import drive
drive.mount('/content/drive')

# Read the dataset using pandas
file = pd.read_csv("/content/drive/My Drive/Motor_Vehicle_Collisions.csv")

# Display the first five rows and describe the dataset
print("First 5 rows of the dataset:")
print(file.head())

print("Row and Column length")
print(file.shape)

desc_stats = file.describe()
print("\nDescriptive Statistics:")
print(desc_stats)


# Milestone 2 - Data Ethics, Pre-Processing, and Exploration


# Find the number of missing values and count them into percentages
missing_values = file.isnull().sum()
missing_values_percentage = (file.isnull().sum() / len(file)) * 100

# Create a DataFrame for missing data
missing_data = pd.DataFrame({
    'Missing Values': missing_values,
    'Missing Values (%)': missing_values_percentage
})
missing_data = missing_data.sort_values(by='Missing Values (%)', ascending=False)

print("\nMissing Values and Their Percentages:")
print(missing_data)


# Plot missing data as a bar chart
plt.figure(figsize=(10, 6))
sns.barplot(x=missing_data.index, y='Missing Values (%)', data=missing_data)
plt.xticks(rotation=90)
plt.title('Percentage of Missing Values by Column')
plt.ylabel('Percentage')
plt.xlabel('Columns')
plt.tight_layout()
plt.show()

# Plot top contributing factors to crashes
top_factors = file['CONTRIBUTING FACTOR VEHICLE 1'].value_counts().head(10)
plt.figure(figsize=(12, 7))
sns.barplot(x=top_factors.values, y=top_factors.index, palette="viridis")
plt.xlabel('Count')
plt.ylabel('Top Contributing Factors')
plt.title('Top 10 Contributing Factors to Crashes')
plt.tight_layout()
plt.show()

"""The top contributing factors to crashes are 1) Driver Inattention, 2)Failure
to Yield Right of Way, and 3) Following too Closely

Looking at this data, it is clear the most vehicle crashes are very much
avoidable. SO, it is important that drivers pay attention to their surroundings,
be ready to give up right of way, and be sure to give enough room when following
another vehicle.
"""

# Plot top vehicle types involved in crashes
top_vehicle_types = file['VEHICLE TYPE CODE 1'].value_counts().head(10)
plt.figure(figsize=(12, 7))
sns.barplot(x=top_vehicle_types.values, y=top_vehicle_types.index, palette="coolwarm")
plt.xlabel('Count')
plt.ylabel('Vehicle Type')
plt.title('Top 10 Vehicle Types Involved in Crashes')
plt.tight_layout()
plt.show()

"""Sedan's, Station Wagons, and Passenger Vehicles are the top vehicles involved
in crashes.
These cars are involved in a larger number of crashes, injuries, and deaths
when compared to the rest of the vehicles simply because there are more of them
on the road."""

"""This chart works well, but does have some things that could be improved.
The x-axis represents the count of crashes each vehicle type has. However,
they are not as informative as they could be. To improve this, we could add
numeric labels for each bar and adjusting the range to fit the data so they
donâ€™t look so compressed.

To the analyst making changes,  It would be helpful to know how many crashes
each vehicle type. More detailed metadata would level this chart up quite a bit."""

# List to store types of crashes
types_of_crashes = {'Pedestrian Injuries': file['NUMBER OF PEDESTRIANS INJURED'].sum(),
    'Cyclist Injuries': file['NUMBER OF CYCLIST INJURED'].sum(),
    'Motorist Injuries': file['NUMBER OF MOTORIST INJURED'].sum(),
    'Pedestrian Deaths': file['NUMBER OF PEDESTRIANS KILLED'].sum(),
    'Cyclist Deaths': file['NUMBER OF CYCLIST KILLED'].sum(),
    'Motorist Deaths': file['NUMBER OF MOTORIST KILLED'].sum()}

# Converting to DataFrame for easier plotting
crash_types_df = pd.DataFrame(list(types_of_crashes.items()), columns=['Crash Type', 'Count'])

# Plot types of crashes
plt.figure(figsize=(12, 7))
sns.barplot(x='Count', y='Crash Type', data=crash_types_df, palette="mako")
plt.title('Types of Crashes and Their Frequencies')
plt.xlabel('Count')
plt.ylabel('Type of Crash')
plt.tight_layout()
plt.show()

"""The chart above that motorist cause the vast majority of of crash.
For this reason, I believe that the Department of Transportation should
focus a vast amount of resources into reducing this number."""


# Milestone 3: Time Series Analysis

"""Time series analysis is the act ofexamining data points recorded at
specific time intervals to identify patterns and trends.
It is used in several fields to make predictions and inform decision-making."""


# Converting 'Crash Date' and 'Crash Time' to datetime format
file['CRASH DATE'] = pd.to_datetime(file['CRASH DATE'])
file['CRASH TIME'] = pd.to_datetime(file['CRASH TIME'], format='%H:%M')

#Analysis of crash by day
file['Hour of Day'] = file['CRASH TIME'].dt.hour

# Group by 'Hour of Day' and count the number of crashes
crashes_per_hour = file.groupby('Hour of Day').size() / file['Hour of Day'].nunique()

#Plotting the average number of crashes
plt.figure(figsize=(10, 6))
sns.barplot(x=crashes_per_hour.index, y=crashes_per_hour.values)
plt.xlabel('Hour of Day')
plt.ylabel('Average Number of Crashes')
plt.xticks(range(0,24))
plt.show()

"""Looking at the data, crashes are at their peak between in the evenings.
Specifically, 4pm is where things are at their peak."""

#Graph to determine how COVID-19 impacted the the number of crashes per month, if at all
file['CRASH DATE'] = pd.to_datetime(file['CRASH DATE'])
monthly_crashes = file.groupby(file['CRASH DATE'].dt.to_period('M')).size()

plt.figure(figsize=(12, 6))
monthly_crashes.plot()
plt.xlabel('Number of Crashes per month')
plt.ylabel('Number of crashes', fontsize= 14)
plt.tight_layout()
plt.show()

"""As the chart demosntrates, when COVID hit its peak and NY closed down,
the number of crashes decreased significantly."""


# Time series decomposition to review trends, seasonality, and residuals

# Count and plot the number of crashes per day, group by CRASH DATE
daily_crashes = file.groupby('CRASH DATE').size()
sns.set(style = 'darkgrid')

plt.figure(figsize=(15, 6))
plt. plot(daily_crashes, label='Daily Crashes')
plt.xlabel('Date')
plt.ylabel('Number of Crashes')
plt.title('Daily Crashes in NYC')
plt.legend()
plt.show()

#Decomposing the data
decomposition = seasonal_decompose(daily_crashes, model='additive', period = 365)

fig, (ax1, ax2, ax3) = plt.subplots(3,1, figsize=(15, 12))
decomposition.trend.plot(ax=ax1)
ax1.set_title('Trend')
decomposition.seasonal.plot(ax=ax2)
ax2.set_title('Seasonal')
decomposition.resid.plot(ax=ax3)
ax3.set_title('Residuals')
plt.tight_layout()
plt.show()

"""Based on the trend graph, we a significant decrease in crashes from 2014 to 2022.
On a similar note, the residuals show a significant fluctuation in 2020.
This is no doubt due to the COVID-19 pandemic."""


# Milestone 4: Geospacial Analysis

# Plotting the distribution of crashes by borough
sns.set_style("whitegrid")


plt.figure(figsize=(12, 7))
borough_count = file['BOROUGH'].value_counts()
sns.barplot(x=borough_count.index, y=borough_count.values, palette="viridis")
plt.title('Distribution of Crashes by Borough', fontsize=16)
plt.xlabel('Borough', fontsize=14)
plt.ylabel('Number of Crashes', fontsize=14)
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

# The bororugh with the highest number of crashes is Brooklyn
# The bororugh with the lowest number of crashes is Staten Island

"""Looking at the data, it stands to reason that a certain burough would have
more crashes due to the fact that it is more populated and has more traffic."""


# Heatmap leveraging the latitude and longitude variables to determine where the most crashes are occurring

data_geo = file.dropna(subset = ['LATITUDE', 'LONGITUDE'])

map = folium.Map(location=[data_geo['LATITUDE'].mean(), data_geo['LONGITUDE'].mean()], zoom_start=12)

heat_data = [[row['LATITUDE'], row['LONGITUDE']] for index, row in data_geo.iterrows()]
HeatMap(heat_data, radius=8, max_zoom=13).add_to(map)

map.save("Heatmap.html")

# The borough with the most concentrated number of crashes is in Manhattan

# Heatmap Continued
sample_data_severity = data_geo.sample(n=1000, random_state=42)

map_severity = folium.Map(location=[40.730610, -73.935242], zoom_start=12)

for index, row in sample_data_severity.iterrows():
    if row['NUMBER OF PERSONS KILLED'] > 0:
        color = "GREEN"  # Fatalities

        folium.features.RegularPolygonMarker(
          location=[row['LATITUDE'], row['LONGTITUDE']],
          number_of_sides=3,
          radius=5,
          gradient = False,
          color=color,
          fill=True,
          fill_color=color
        ).add_to(map_severity)


    elif row['NUMBER OF PERSONS INJURED'] > 0:
        color = "RED"  # Injuries
        folium.CircleMarker(
          location=[row['LATITUDE'], row['LONGITUDE']],
          radius=5,
          color=color,
          fill=True,
          fill_color=color
       ).add_to(map_severity)
    else:
        color = "BLUE"  # No injuries or fatalities
        folium.features.RegularPolygonMarker(
          location=[row['LATITUDE'], row['LONGITUDE']],
          number_of_sides=4,
          radius=5,
          gradient = False,
          color=color,
          fill=True,
          fill_color=color
        ).add_to(map_severity)


map_severity.save("severity.html")

"""The most dangeorus intersections those merging into WB bridge and the roads
going to JFK and LaGuardia airports."""

